{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 概述\n",
    "- 本文的重点就是讲解 Native 层服务注册的过程\n",
    "\n",
    "### 1.1 media 服务注册\n",
    "- media 入口函数是 main_mediaserver.cpp 中的 main() 方法\n",
    "  - 获取 ServiceManager: 讲解了 defaultServiceManager() 返回的是 BpServiceManager 对象，用于跟 ServiceManager 进程通信;\n",
    "  - 理解 Binder 线程池的管理，讲解了 startThreadPool 和 joinThreadPool 过程。\n",
    "\n",
    "```\n",
    "int main(int argc __unused, char** argv)\n",
    "{\n",
    "    ...\n",
    "    InitializeIcuOrDie();\n",
    "    // 获得ProcessState实例对象【见小节2.1】\n",
    "    sp<ProcessState> proc(ProcessState::self());\n",
    "    \n",
    "    // 获取 BpServiceManager 对象\n",
    "    sp<IServiceManager> sm = defaultServiceManager();\n",
    "    AudioFlinger::instantiate();\n",
    "    \n",
    "    // 注册多媒体服务  【见小节3.1】\n",
    "    MediaPlayerService::instantiate();\n",
    "    ResourceManagerService::instantiate();\n",
    "    CameraService::instantiate();\n",
    "    AudioPolicyService::instantiate();\n",
    "    SoundTriggerHwService::instantiate();\n",
    "    RadioService::instantiate();\n",
    "    registerExtensions();\n",
    "    \n",
    "    // 启动 Binder 线程池\n",
    "    ProcessState::self()->startThreadPool();\n",
    "    \n",
    "    // 当前线程加入到线程池\n",
    "    IPCThreadState::self()->joinThreadPool();\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 类图\n",
    "\n",
    "![image](binder5_page1.png)\n",
    "\n",
    "- 图解：\n",
    "  - 蓝色代表的是注册 MediaPlayerService 服务所涉及的类\n",
    "  - 绿色代表的是 Binder 架构中与 Binder 驱动通信过程中的最为核心的两个类\n",
    "  - 紫色代表的是注册服务和获取服务的公共接口/父类\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 时序图\n",
    "\n",
    "![image](binder5_page2.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 ProcessState\n",
    "### 2.1 ProcessState::self\n",
    "- 获得 ProcessState 对象: 这也是单例模式，从而保证每一个进程只有一个 ProcessState 对象。其中 gProcess 和 gProcessMutex 是保存在Static.cpp 类的全局变量\n",
    "\n",
    "```\n",
    "sp<ProcessState> ProcessState::self()\n",
    "{\n",
    "    Mutex::Autolock _l(gProcessMutex);\n",
    "    if (gProcess != NULL) {\n",
    "        return gProcess;\n",
    "    }\n",
    "\n",
    "    //实例化ProcessState 【见小节2.2】\n",
    "    gProcess = new ProcessState;\n",
    "    return gProcess;\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ProcessState 初始化\n",
    "- ProcessState 的单例模式的惟一性，因此一个进程只打开 binder 设备一次,其中 ProcessState 的成员变量 mDriverFD 记录 binder 驱动的 fd，用于访问 binder 设备\n",
    "- BINDER_VM_SIZE = (1*1024*1024) - (4096 *2), binder 分配的默认内存大小为 1M-8k\n",
    "- DEFAULT_MAX_BINDER_THREADS = 15，binder 默认的最大可并发访问的线程数为 16\n",
    "\n",
    "```\n",
    "ProcessState::ProcessState()\n",
    "    : mDriverFD(open_driver()) // 打开 Binder 驱动【见小节2.3】\n",
    "    , mVMStart(MAP_FAILED)\n",
    "    , mThreadCountLock(PTHREAD_MUTEX_INITIALIZER)\n",
    "    , mThreadCountDecrement(PTHREAD_COND_INITIALIZER)\n",
    "    , mExecutingThreadsCount(0)\n",
    "    , mMaxThreads(DEFAULT_MAX_BINDER_THREADS)\n",
    "    , mManagesContexts(false)\n",
    "    , mBinderContextCheckFunc(NULL)\n",
    "    , mBinderContextUserData(NULL)\n",
    "    , mThreadPoolStarted(false)\n",
    "    , mThreadPoolSeq(1)\n",
    "{\n",
    "    if (mDriverFD >= 0) {\n",
    "        // 采用内存映射函数 mmap，给 binder 分配一块虚拟地址空间【见小节2.4】\n",
    "        mVMStart = mmap(0, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0);\n",
    "        if (mVMStart == MAP_FAILED) {\n",
    "            close(mDriverFD);  // 没有足够空间分配给/dev/binder, 则关闭驱动\n",
    "            mDriverFD = -1;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 open_driver\n",
    "- open_driver 作用是打开 /dev/binder 设备，设定 binder 支持的最大线程数\n",
    "\n",
    "```\n",
    "static int open_driver()\n",
    "{\n",
    "    // 打开 /dev/binder 设备，建立与内核的 Binder 驱动的交互通道\n",
    "    int fd = open(\"/dev/binder\", O_RDWR);\n",
    "    if (fd >= 0) {\n",
    "        fcntl(fd, F_SETFD, FD_CLOEXEC);\n",
    "        int vers = 0;\n",
    "        status_t result = ioctl(fd, BINDER_VERSION, &vers);\n",
    "        if (result == -1) {\n",
    "            close(fd);\n",
    "            fd = -1;\n",
    "        }\n",
    "        if (result != 0 || vers != BINDER_CURRENT_PROTOCOL_VERSION) {\n",
    "            close(fd);\n",
    "            fd = -1;\n",
    "        }\n",
    "        size_t maxThreads = DEFAULT_MAX_BINDER_THREADS;\n",
    "\n",
    "        // 通过 ioctl 设置 binder 驱动，能支持的最大线程数\n",
    "        result = ioctl(fd, BINDER_SET_MAX_THREADS, &maxThreads);\n",
    "        if (result == -1) {\n",
    "            ALOGE(\"Binder ioctl to set max threads failed: %s\", strerror(errno));\n",
    "        }\n",
    "    } else {\n",
    "        ALOGW(\"Opening '/dev/binder' failed: %s\\n\", strerror(errno));\n",
    "    }\n",
    "    return fd;\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 mmap\n",
    "```\n",
    "// 原型\n",
    "// 此处 mmap(0, BINDER_VM_SIZE, PROT_READ, MAP_PRIVATE | MAP_NORESERVE, mDriverFD, 0);\n",
    "void* mmap(void* addr, size_t size, int prot, int flags, int fd, off_t offset) \n",
    "```\n",
    "\n",
    "- 参数说明：\n",
    "  - addr: 代表映射到进程地址空间的起始地址，当值等于 0 则由内核选择合适地址，此处为 0\n",
    "  - size: 代表需要映射的内存地址空间的大小，此处为 1M-8K\n",
    "  - prot: 代表内存映射区的读写等属性值，此处为PROT_READ(可读取)\n",
    "  - flags: 标志位，此处为 MAP_PRIVATE(私有映射，多进程间不共享内容的改变) 和 MAP_NORESERVE(不保留交换空间)\n",
    "  - fd: 代表 mmap 所关联的文件描述符，此处为 mDriverFD\n",
    "  - offset：偏移量，此处为 0\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 服务注册\n",
    "### 3.1 instantiate\n",
    "```\n",
    "void MediaPlayerService::instantiate() {\n",
    "    // 注册服务【见小节3.2】\n",
    "    defaultServiceManager()->addService(String16(\"media.player\"), new MediaPlayerService());\n",
    "}\n",
    "```\n",
    "\n",
    "- 注册服务 MediaPlayerService：由 defaultServiceManager() 返回的是 BpServiceManager，同时会创建 ProcessState 对象和 BpBinder 对象。故此处等价于调用 BpServiceManager->addService。\n",
    "- MediaPlayerService 位于 libmediaplayerservice 库。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 BpSM.addService\n",
    "- 服务注册过程：向 ServiceManager 注册服务 MediaPlayerService，服务名为 \"media.player\"\n",
    "\n",
    "```\n",
    "virtual status_t addService(const String16& name, const sp<IBinder>& service, bool allowIsolated) {\n",
    "    Parcel data, reply; // Parcel 是数据通信包\n",
    "    // 写入头信息 \"android.os.IServiceManager\"\n",
    "    data.writeInterfaceToken(IServiceManager::getInterfaceDescriptor());\n",
    "    data.writeString16(name);        // name 为 \"media.player\"\n",
    "    data.writeStrongBinder(service); // MediaPlayerService 对象【见小节3.2.1】\n",
    "    data.writeInt32(allowIsolated ? 1 : 0); // allowIsolated = false\n",
    "    // remote() 指向的是 BpBinder 对象【见小节3.3】\n",
    "    status_t err = remote()->transact(ADD_SERVICE_TRANSACTION, data, &reply);\n",
    "    return err == NO_ERROR ? reply.readExceptionCode() : err;\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3.2.1 writeStrongBinder\n",
    "```\n",
    "status_t Parcel::writeStrongBinder(const sp<IBinder>& val)\n",
    "{\n",
    "    return flatten_binder(ProcessState::self(), val, this);\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3.2.2 flatten_binder\n",
    "```\n",
    "status_t flatten_binder(const sp<ProcessState>& /*proc*/,\n",
    "    const sp<IBinder>& binder, Parcel* out)\n",
    "{\n",
    "    flat_binder_object obj;\n",
    "\n",
    "    obj.flags = 0x7f | FLAT_BINDER_FLAG_ACCEPTS_FDS;\n",
    "    if (binder != NULL) {\n",
    "        IBinder *local = binder->localBinder(); // 本地 Binder 不为空\n",
    "        if (!local) {\n",
    "            BpBinder *proxy = binder->remoteBinder();\n",
    "            const int32_t handle = proxy ? proxy->handle() : 0;\n",
    "            obj.type = BINDER_TYPE_HANDLE;\n",
    "            obj.binder = 0;\n",
    "            obj.handle = handle;\n",
    "            obj.cookie = 0;\n",
    "        } else {\n",
    "            // 进入该分支\n",
    "            obj.type = BINDER_TYPE_BINDER;\n",
    "            obj.binder = reinterpret_cast<uintptr_t>(local->getWeakRefs());\n",
    "            obj.cookie = reinterpret_cast<uintptr_t>(local);\n",
    "        }\n",
    "    } else {\n",
    "        ...\n",
    "    }\n",
    "    //【见小节3.2.3】\n",
    "    return finish_flatten_binder(binder, obj, out);\n",
    "}\n",
    "```\n",
    "\n",
    "- 将 Binder 对象扁平化，转换成 flat_binder_object 对象\n",
    "  - 对于 Binder 实体，则 cookie 记录 Binder 实体的指针\n",
    "  - 对于 Binder 代理，则用 handle 记录 Binder 代理的句柄\n",
    "\n",
    "```\n",
    "BBinder* BBinder::localBinder()\n",
    "{\n",
    "    return this;\n",
    "}\n",
    "\n",
    "BBinder* IBinder::localBinder()\n",
    "{\n",
    "    return NULL;\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3.2.3 finish_flatten_binder\n",
    "- 将 flat_binder_object 写入 out\n",
    "\n",
    "```\n",
    "inline static status_t finish_flatten_binder(\n",
    "    const sp<IBinder>& , const flat_binder_object& flat, Parcel* out)\n",
    "{\n",
    "    return out->writeObject(flat, false);\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 BpBinder::transact\n",
    "- Binder 代理类调用 transact() 方法，真正工作还是交给 IPCThreadState 来进行 transact 工作\n",
    "\n",
    "```\n",
    "status_t BpBinder::transact(uint32_t code, const Parcel& data, Parcel* reply, uint32_t flags)\n",
    "{\n",
    "    if (mAlive) {\n",
    "        // code = ADD_SERVICE_TRANSACTION【见小节3.4】\n",
    "        status_t status = IPCThreadState::self()->transact(mHandle, code, data, reply, flags);\n",
    "        if (status == DEAD_OBJECT) mAlive = 0;\n",
    "        return status;\n",
    "    }\n",
    "    return DEAD_OBJECT;\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3.3.1 IPCThreadState::self\n",
    "- TLS 是指 Thread local storage(线程本地储存空间)，每个线程都拥有自己的 TLS，并且是私有空间，线程之间不会共享\n",
    "- 通过 pthread_getspecific/pthread_setspecific 函数可以获取/设置这些空间中的内容。从线程本地存储空间中获得保存在其中的 IPCThreadState 对象\n",
    "\n",
    "```\n",
    "IPCThreadState* IPCThreadState::self()\n",
    "{\n",
    "    if (gHaveTLS) {\n",
    "restart:\n",
    "        const pthread_key_t k = gTLS;\n",
    "        IPCThreadState* st = (IPCThreadState*)pthread_getspecific(k);\n",
    "        if (st) return st;\n",
    "        return new IPCThreadState;  // 初始 IPCThreadState 【见小节3.3.2】\n",
    "    }\n",
    "\n",
    "    if (gShutdown) return NULL;\n",
    "\n",
    "    pthread_mutex_lock(&gTLSMutex);\n",
    "    if (!gHaveTLS) {\n",
    "        // 首次进入 gHaveTLS 为 false\n",
    "        if (pthread_key_create(&gTLS, threadDestructor) != 0) {\n",
    "            // 创建线程的 TLS\n",
    "            pthread_mutex_unlock(&gTLSMutex);\n",
    "            return NULL;\n",
    "        }\n",
    "        gHaveTLS = true;\n",
    "    }\n",
    "    pthread_mutex_unlock(&gTLSMutex);\n",
    "    goto restart;\n",
    "}\n",
    "```\n",
    "\n",
    "#### 3.3.2 IPCThreadState 初始化\n",
    "- 每个线程都有一个 IPCThreadState，每个 IPCThreadState 中都有一个 mIn、一个 mOut。成员变量 mProcess 保存了 ProcessState 变量(每个进程只有一个)\n",
    "  - mIn 用来接收来自 Binder 设备的数据，默认大小为 256 字节\n",
    "  - mOut 用来存储发往 Binder 设备的数据，默认大小为 256 字节\n",
    "\n",
    "```\n",
    "IPCThreadState::IPCThreadState()\n",
    "    : mProcess(ProcessState::self()),\n",
    "      mMyThreadId(gettid()),\n",
    "      mStrictModePolicy(0),\n",
    "      mLastTransactionBinderFlags(0)\n",
    "{\n",
    "    pthread_setspecific(gTLS, this);\n",
    "    clearCaller();\n",
    "    mIn.setDataCapacity(256);\n",
    "    mOut.setDataCapacity(256);\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 IPC::transact\n",
    "- IPCThreadState 进行 transact 事务处理分3部分\n",
    "  - errorCheck() // 数据错误检查\n",
    "  - writeTransactionData() // 传输数据\n",
    "  - waitForResponse() // 等待响应\n",
    "\n",
    "```\n",
    "status_t IPCThreadState::transact(int32_t handle,\n",
    "                                  uint32_t code, const Parcel& data,\n",
    "                                  Parcel* reply, uint32_t flags)\n",
    "{\n",
    "    status_t err = data.errorCheck(); // 数据错误检查\n",
    "    flags |= TF_ACCEPT_FDS;\n",
    "    ....\n",
    "    if (err == NO_ERROR) {\n",
    "        // 传输数据 【见小节3.5】\n",
    "        err = writeTransactionData(BC_TRANSACTION, flags, handle, code, data, NULL);\n",
    "    }\n",
    "    ...\n",
    "\n",
    "    if ((flags & TF_ONE_WAY) == 0) {\n",
    "        if (reply) {\n",
    "            // 等待响应 【见小节3.6】\n",
    "            err = waitForResponse(reply);\n",
    "        } else {\n",
    "            Parcel fakeReply;\n",
    "            err = waitForResponse(&fakeReply);\n",
    "        }\n",
    "\n",
    "    } else {\n",
    "        // oneway，则不需要等待 reply 的场景\n",
    "        err = waitForResponse(NULL, NULL);\n",
    "    }\n",
    "    return err;\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 IPC.writeTransactionData\n",
    "- 其中 handle 的值用来标识目的端，注册服务过程的目的端为 service manager，此处 handle=0 所对应的是 binder_context_mgr_node 对象，正是 service manager 所对应的 binder 实体对象\n",
    "- binder_transaction_data 结构体是 binder 驱动通信的数据结构，该过程最终是把 Binder 请求码 BC_TRANSACTION 和binder_transaction_data 结构体写入到 mOut\n",
    "- transact 过程，先写完 binder_transaction_data 数据，其中 Parcel data 的重要成员变量\n",
    "  - mDataSize: 保存在 data_size，binder_transaction 的数据大小\n",
    "  - mData: 保存在 ptr.buffer, binder_transaction 的数据的起始地址\n",
    "  - mObjectsSize: 保存在 ptr.offsets_size，记录着 flat_binder_object 结构体的个数\n",
    "  - mObjects: 保存在 offsets, 记录着 flat_binder_object 结构体在数据偏移量\n",
    "\n",
    "```\n",
    "status_t IPCThreadState::writeTransactionData(int32_t cmd, uint32_t binderFlags,\n",
    "    int32_t handle, uint32_t code, const Parcel& data, status_t* statusBuffer)\n",
    "{\n",
    "    binder_transaction_data tr;\n",
    "    tr.target.ptr = 0;\n",
    "    tr.target.handle = handle; // handle = 0\n",
    "    tr.code = code;            // code = ADD_SERVICE_TRANSACTION\n",
    "    tr.flags = binderFlags;    // binderFlags = 0\n",
    "    tr.cookie = 0;\n",
    "    tr.sender_pid = 0;\n",
    "    tr.sender_euid = 0;\n",
    "\n",
    "    // data 为记录 Media 服务信息的 Parcel 对象\n",
    "    const status_t err = data.errorCheck();\n",
    "    if (err == NO_ERROR) {\n",
    "        tr.data_size = data.ipcDataSize();        // mDataSize\n",
    "        tr.data.ptr.buffer = data.ipcData();      // mData\n",
    "        tr.offsets_size = data.ipcObjectsCount()*sizeof(binder_size_t); // mObjectsSize\n",
    "        tr.data.ptr.offsets = data.ipcObjects();  // mObjects\n",
    "    } else if (statusBuffer) {\n",
    "        ...\n",
    "    } else {\n",
    "        return (mLastError = err);\n",
    "    }\n",
    "\n",
    "    mOut.writeInt32(cmd);         // cmd = BC_TRANSACTION\n",
    "    mOut.write(&tr, sizeof(tr));  // 写入 binder_transaction_data 数据\n",
    "    return NO_ERROR;\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 IPC.waitForResponse\n",
    "- 在 waitForResponse 过程, 首先执行 BR_TRANSACTION_COMPLETE\n",
    "- 另外，目标进程收到事务后，处理 BR_TRANSACTION 事务。 然后发送给当前进程，再执行 BR_REPLY 命令\n",
    "\n",
    "```\n",
    "status_t IPCThreadState::waitForResponse(Parcel *reply, status_t *acquireResult)\n",
    "{\n",
    "    int32_t cmd;\n",
    "    int32_t err;\n",
    "    while (1) {\n",
    "        if ((err=talkWithDriver()) < NO_ERROR) break; // 【见小节3.7】\n",
    "        ...\n",
    "        if (mIn.dataAvail() == 0) continue;\n",
    "\n",
    "        cmd = mIn.readInt32();\n",
    "        switch (cmd) {\n",
    "            case BR_TRANSACTION_COMPLETE: ...\n",
    "            case BR_DEAD_REPLY: ...\n",
    "            case BR_FAILED_REPLY: ...\n",
    "            case BR_ACQUIRE_RESULT: ...\n",
    "            case BR_REPLY: ...\n",
    "                goto finish;\n",
    "\n",
    "            default:\n",
    "                err = executeCommand(cmd);  //【见小节3.x】\n",
    "                if (err != NO_ERROR) goto finish;\n",
    "                break;\n",
    "        }\n",
    "    }\n",
    "    ...\n",
    "    return err;\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 IPC.talkWithDriver\n",
    "- binder_write_read 结构体用来与 Binder 设备交换数据的结构, 通过 ioctl 与 mDriverFD 通信，是真正与 Binder 驱动进行数据读写交互的过程。主要是操作 mOut 和 mIn 变量。\n",
    "\n",
    "```\n",
    "status_t IPCThreadState::talkWithDriver(bool doReceive)\n",
    "{\n",
    "    ...\n",
    "    binder_write_read bwr;\n",
    "    const bool needRead = mIn.dataPosition() >= mIn.dataSize();\n",
    "    const size_t outAvail = (!doReceive || needRead) ? mOut.dataSize() : 0;\n",
    "\n",
    "    bwr.write_size = outAvail;\n",
    "    bwr.write_buffer = (uintptr_t)mOut.data();\n",
    "\n",
    "    if (doReceive && needRead) {\n",
    "        // 接收数据缓冲区信息的填充。如果以后收到数据，就直接填在 mIn 中了。\n",
    "        bwr.read_size = mIn.dataCapacity();\n",
    "        bwr.read_buffer = (uintptr_t)mIn.data();\n",
    "    } else {\n",
    "        bwr.read_size = 0;\n",
    "        bwr.read_buffer = 0;\n",
    "    }\n",
    "    // 当读缓冲和写缓冲都为空，则直接返回\n",
    "    if ((bwr.write_size == 0) && (bwr.read_size == 0)) return NO_ERROR;\n",
    "\n",
    "    bwr.write_consumed = 0;\n",
    "    bwr.read_consumed = 0;\n",
    "    status_t err;\n",
    "    do {\n",
    "        // 通过 ioctl 不停的读写操作，跟 Binder Driver 进行通信\n",
    "        if (ioctl(mProcess->mDriverFD, BINDER_WRITE_READ, &bwr) >= 0)\n",
    "            err = NO_ERROR;\n",
    "        ...\n",
    "    } while (err == -EINTR); // 当被中断，则继续执行\n",
    "    ...\n",
    "    return err;\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Binder Driver\n",
    "### 4.1 binder_ioctl_write_read\n",
    "```\n",
    "static int binder_ioctl_write_read(struct file *filp,\n",
    "                unsigned int cmd, unsigned long arg,\n",
    "                struct binder_thread *thread)\n",
    "{\n",
    "    struct binder_proc *proc = filp->private_data;\n",
    "    void __user *ubuf = (void __user *)arg;\n",
    "    struct binder_write_read bwr;\n",
    "\n",
    "    // 将用户空间 bwr 结构体拷贝到内核空间\n",
    "    copy_from_user(&bwr, ubuf, sizeof(bwr));\n",
    "    ...\n",
    "\n",
    "    if (bwr.write_size > 0) {\n",
    "        // 将数据放入目标进程【见小节4.2】\n",
    "        ret = binder_thread_write(proc, thread,\n",
    "                      bwr.write_buffer,\n",
    "                      bwr.write_size,\n",
    "                      &bwr.write_consumed);\n",
    "        ...\n",
    "    }\n",
    "    if (bwr.read_size > 0) {\n",
    "        // 读取自己队列的数据 【见小节】\n",
    "        ret = binder_thread_read(proc, thread, bwr.read_buffer,\n",
    "             bwr.read_size,\n",
    "             &bwr.read_consumed,\n",
    "             filp->f_flags & O_NONBLOCK);\n",
    "        if (!list_empty(&proc->todo))\n",
    "            wake_up_interruptible(&proc->wait);\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    // 将内核空间 bwr 结构体拷贝到用户空间\n",
    "    copy_to_user(ubuf, &bwr, sizeof(bwr));\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 binder_thread_write\n",
    "```\n",
    "static int binder_thread_write(struct binder_proc *proc,\n",
    "            struct binder_thread *thread,\n",
    "            binder_uintptr_t binder_buffer, size_t size,\n",
    "            binder_size_t *consumed)\n",
    "{\n",
    "    uint32_t cmd;\n",
    "    void __user *buffer = (void __user *)(uintptr_t)binder_buffer;\n",
    "    void __user *ptr = buffer + *consumed;\n",
    "    void __user *end = buffer + size;\n",
    "    while (ptr < end && thread->return_error == BR_OK) {\n",
    "        // 拷贝用户空间的 cmd 命令，此时为 BC_TRANSACTION\n",
    "        if (get_user(cmd, (uint32_t __user *)ptr)) -EFAULT;\n",
    "        ptr += sizeof(uint32_t);\n",
    "        switch (cmd) {\n",
    "        case BC_TRANSACTION:\n",
    "        case BC_REPLY: {\n",
    "            struct binder_transaction_data tr;\n",
    "            // 拷贝用户空间的 binder_transaction_data\n",
    "            if (copy_from_user(&tr, ptr, sizeof(tr)))   return -EFAULT;\n",
    "            ptr += sizeof(tr);\n",
    "            // 见小节4.3】\n",
    "            binder_transaction(proc, thread, &tr, cmd == BC_REPLY);\n",
    "            break;\n",
    "        }\n",
    "        ...\n",
    "    }\n",
    "    *consumed = ptr - buffer;\n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 binder_transaction\n",
    "```\n",
    "static void binder_transaction(struct binder_proc *proc,\n",
    "               struct binder_thread *thread,\n",
    "               struct binder_transaction_data *tr, int reply){\n",
    "    struct binder_transaction *t;\n",
    "   \tstruct binder_work *tcomplete;\n",
    "    ...\n",
    "\n",
    "    if (reply) {\n",
    "        ...\n",
    "    } else {\n",
    "        if (tr->target.handle) {\n",
    "            ...\n",
    "        } else {\n",
    "            // handle = 0 则找到 servicemanager 实体\n",
    "            target_node = binder_context_mgr_node;\n",
    "        }\n",
    "        // target_proc 为 servicemanager 进程\n",
    "        target_proc = target_node->proc;\n",
    "    }\n",
    "\n",
    "    if (target_thread) {\n",
    "        ...\n",
    "    } else {\n",
    "        // 找到 servicemanager 进程的 todo 队列\n",
    "        target_list = &target_proc->todo;\n",
    "        target_wait = &target_proc->wait;\n",
    "    }\n",
    "\n",
    "    t = kzalloc(sizeof(*t), GFP_KERNEL);\n",
    "    tcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n",
    "\n",
    "    // 非 oneway 的通信方式，把当前 thread 保存到 transaction 的 from 字段\n",
    "    if (!reply && !(tr->flags & TF_ONE_WAY))\n",
    "        t->from = thread;\n",
    "    else\n",
    "        t->from = NULL;\n",
    "\n",
    "    t->sender_euid = task_euid(proc->tsk);\n",
    "    t->to_proc = target_proc;    // 此次通信目标进程为 servicemanager 进程\n",
    "    t->to_thread = target_thread;\n",
    "    t->code = tr->code;          // 此次通信 code = ADD_SERVICE_TRANSACTION\n",
    "    t->flags = tr->flags;        // 此次通信 flags = 0\n",
    "    t->priority = task_nice(current);\n",
    "\n",
    "    // 从 servicemanager 进程中分配 buffer\n",
    "    t->buffer = binder_alloc_buf(target_proc, tr->data_size,\n",
    "        tr->offsets_size, !reply && (t->flags & TF_ONE_WAY));\n",
    "\n",
    "    t->buffer->allow_user_free = 0;\n",
    "    t->buffer->transaction = t;\n",
    "    t->buffer->target_node = target_node;\n",
    "\n",
    "    if (target_node)\n",
    "        binder_inc_node(target_node, 1, 0, NULL);   // 引用计数加1\n",
    "    offp = (binder_size_t *)(t->buffer->data + ALIGN(tr->data_size, sizeof(void *)));\n",
    "\n",
    "    // 分别拷贝用户空间的 binder_transaction_data 中 ptr.buffer 和 ptr.offsets 到内核\n",
    "    copy_from_user(t->buffer->data,\n",
    "        (const void __user *)(uintptr_t)tr->data.ptr.buffer, tr->data_size);\n",
    "    copy_from_user(offp,\n",
    "        (const void __user *)(uintptr_t)tr->data.ptr.offsets, tr->offsets_size);\n",
    "\n",
    "    off_end = (void *)offp + tr->offsets_size;\n",
    "\n",
    "    for (; offp < off_end; offp++) {\n",
    "        struct flat_binder_object *fp;\n",
    "        fp = (struct flat_binder_object *)(t->buffer->data + *offp);\n",
    "        off_min = *offp + sizeof(struct flat_binder_object);\n",
    "        switch (fp->type) {\n",
    "            case BINDER_TYPE_BINDER:\n",
    "            case BINDER_TYPE_WEAK_BINDER: {\n",
    "              struct binder_ref *ref;\n",
    "              //【见4.3.1】\n",
    "              struct binder_node *node = binder_get_node(proc, fp->binder);\n",
    "              if (node == NULL) {\n",
    "                // 服务所在进程 创建 binder_node 实体【见4.3.2】\n",
    "                node = binder_new_node(proc, fp->binder, fp->cookie);\n",
    "                ...\n",
    "              }\n",
    "              // servicemanager 进程 binder_ref【见4.3.3】\n",
    "              ref = binder_get_ref_for_node(target_proc, node);\n",
    "              ...\n",
    "              // 调整 type 为 HANDLE 类型\n",
    "              if (fp->type == BINDER_TYPE_BINDER)\n",
    "                fp->type = BINDER_TYPE_HANDLE;\n",
    "              else\n",
    "                fp->type = BINDER_TYPE_WEAK_HANDLE;\n",
    "              fp->binder = 0;\n",
    "              fp->handle = ref->desc;    // 设置 handle 值\n",
    "              fp->cookie = 0;\n",
    "              binder_inc_ref(ref, fp->type == BINDER_TYPE_HANDLE,\n",
    "                       &thread->todo);\n",
    "            } break;\n",
    "            case :...\n",
    "    }\n",
    "\n",
    "    if (reply) {\n",
    "        ..\n",
    "    } else if (!(t->flags & TF_ONE_WAY)) {\n",
    "        // BC_TRANSACTION 且 非 oneway, 则设置事务栈信息\n",
    "        t->need_reply = 1;\n",
    "        t->from_parent = thread->transaction_stack;\n",
    "        thread->transaction_stack = t;\n",
    "    } else {\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    // 将 BINDER_WORK_TRANSACTION 添加到目标队列，本次通信的目标队列为 target_proc->todo\n",
    "    t->work.type = BINDER_WORK_TRANSACTION;\n",
    "    list_add_tail(&t->work.entry, target_list);\n",
    "\n",
    "    // 将 BINDER_WORK_TRANSACTION_COMPLETE 添加到当前线程的 todo 队列\n",
    "    tcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n",
    "    list_add_tail(&tcomplete->entry, &thread->todo);\n",
    "\n",
    "    // 唤醒等待队列，本次通信的目标队列为 target_proc->wait\n",
    "    if (target_wait)\n",
    "        wake_up_interruptible(target_wait);\n",
    "    return;\n",
    "}\n",
    "```\n",
    "\n",
    "- 注册服务的过程，传递的是 BBinder 对象，故 (小节3.2.1) 的 writeStrongBinder() 过程中 localBinder 不为空， 从而 flat_binder_object.type 等于 BINDER_TYPE_BINDER\n",
    "- 服务注册过程是在服务所在进程创建 binder_node，在 servicemanager 进程创建 binder_ref。 对于同一个 binder_node，每个进程只会创建一个 binder_ref 对象\n",
    "- 向 servicemanager 的 binder_proc->todo 添加 BINDER_WORK_TRANSACTION 事务，接下来进入 ServiceManager 进程\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 binder_get_node\n",
    "- 从 binder_proc 来根据 binder 指针 ptr 值，查询相应的 binder_node\n",
    "\n",
    "```\n",
    "static struct binder_node *binder_get_node(struct binder_proc *proc,\n",
    "             binder_uintptr_t ptr)\n",
    "{\n",
    "  struct rb_node *n = proc->nodes.rb_node;\n",
    "  struct binder_node *node;\n",
    "\n",
    "  while (n) {\n",
    "    node = rb_entry(n, struct binder_node, rb_node);\n",
    "\n",
    "    if (ptr < node->ptr)\n",
    "      n = n->rb_left;\n",
    "    else if (ptr > node->ptr)\n",
    "      n = n->rb_right;\n",
    "    else\n",
    "      return node;\n",
    "  }\n",
    "  return NULL;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 binder_new_node\n",
    "```\n",
    "static struct binder_node *binder_new_node(struct binder_proc *proc,\n",
    "                       binder_uintptr_t ptr,\n",
    "                       binder_uintptr_t cookie)\n",
    "{\n",
    "    struct rb_node **p = &proc->nodes.rb_node;\n",
    "    struct rb_node *parent = NULL;\n",
    "    struct binder_node *node;\n",
    "    ... //红黑树位置查找\n",
    "\n",
    "    //给新创建的binder_node 分配内核空间\n",
    "    node = kzalloc(sizeof(*node), GFP_KERNEL);\n",
    "\n",
    "    // 将新创建的node添加到proc红黑树；\n",
    "    rb_link_node(&node->rb_node, parent, p);\n",
    "    rb_insert_color(&node->rb_node, &proc->nodes);\n",
    "    node->debug_id = ++binder_last_id;\n",
    "    node->proc = proc;\n",
    "    node->ptr = ptr;\n",
    "    node->cookie = cookie;\n",
    "    node->work.type = BINDER_WORK_NODE; //设置binder_work的type\n",
    "    INIT_LIST_HEAD(&node->work.entry);\n",
    "    INIT_LIST_HEAD(&node->async_todo);\n",
    "    return node;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.3 binder_get_ref_for_node\n",
    "- handle值计算方法规律：\n",
    "  - 每个进程 binder_proc 所记录的 binder_ref 的 handle 值是从1开始递增的\n",
    "  - 所有进程 binder_proc 所记录的 handle = 0 的 binder_ref 都指向 service manager\n",
    "  - 同一个服务的 binder_node 在不同进程的 binder_ref 的 handle 值可以不同\n",
    "\n",
    "```\n",
    "static struct binder_ref *binder_get_ref_for_node(struct binder_proc *proc,\n",
    "              struct binder_node *node)\n",
    "{\n",
    "  struct rb_node *n;\n",
    "  struct rb_node **p = &proc->refs_by_node.rb_node;\n",
    "  struct rb_node *parent = NULL;\n",
    "  struct binder_ref *ref, *new_ref;\n",
    "  // 从 refs_by_node 红黑树，找到 binder_ref 则直接返回。\n",
    "  while (*p) {\n",
    "    parent = *p;\n",
    "    ref = rb_entry(parent, struct binder_ref, rb_node_node);\n",
    "\n",
    "    if (node < ref->node)\n",
    "      p = &(*p)->rb_left;\n",
    "    else if (node > ref->node)\n",
    "      p = &(*p)->rb_right;\n",
    "    else\n",
    "      return ref;\n",
    "  }\n",
    "\n",
    "  // 创建 binder_ref\n",
    "  new_ref = kzalloc_preempt_disabled(sizeof(*ref));\n",
    "\n",
    "  new_ref->debug_id = ++binder_last_id;\n",
    "  new_ref->proc = proc;   // 记录进程信息\n",
    "  new_ref->node = node;   // 记录 binder 节点\n",
    "  rb_link_node(&new_ref->rb_node_node, parent, p);\n",
    "  rb_insert_color(&new_ref->rb_node_node, &proc->refs_by_node);\n",
    "\n",
    "  // 计算 binder 引用的 handle 值，该值返回给 target_proc 进程\n",
    "  new_ref->desc = (node == binder_context_mgr_node) ? 0 : 1;\n",
    "  // 从红黑树最最左边的 handle 对比，依次递增，直到红黑树遍历结束或者找到更大的 handle 则结束。\n",
    "  for (n = rb_first(&proc->refs_by_desc); n != NULL; n = rb_next(n)) {\n",
    "    // 根据 binder_ref 的成员变量 rb_node_desc 的地址指针 n，来获取 binder_ref 的首地址\n",
    "    ref = rb_entry(n, struct binder_ref, rb_node_desc);\n",
    "    if (ref->desc > new_ref->desc)\n",
    "      break;\n",
    "    new_ref->desc = ref->desc + 1;\n",
    "  }\n",
    "\n",
    "  // 将新创建的 new_ref 插入 proc->refs_by_desc 红黑树\n",
    "  p = &proc->refs_by_desc.rb_node;\n",
    "  while (*p) {\n",
    "    parent = *p;\n",
    "    ref = rb_entry(parent, struct binder_ref, rb_node_desc);\n",
    "\n",
    "    if (new_ref->desc < ref->desc)\n",
    "      p = &(*p)->rb_left;\n",
    "    else if (new_ref->desc > ref->desc)\n",
    "      p = &(*p)->rb_right;\n",
    "    else\n",
    "      BUG();\n",
    "  }\n",
    "  rb_link_node(&new_ref->rb_node_desc, parent, p);\n",
    "  rb_insert_color(&new_ref->rb_node_desc, &proc->refs_by_desc);\n",
    "  if (node) {\n",
    "    hlist_add_head(&new_ref->node_entry, &node->refs);\n",
    "  }\n",
    "  return new_ref;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ServiceManager\n",
    "- 由 Binder系列3 — 启动 ServiceManager 已介绍其原理，循环在 binder_loop() 过程， 会调用 binder_parse() 方法\n",
    "\n",
    "### 5.1 binder_parse\n",
    "```\n",
    "int binder_parse(struct binder_state *bs, struct binder_io *bio, uintptr_t ptr, size_t size, binder_handler func) {\n",
    "    int r = 1;\n",
    "    uintptr_t end = ptr + (uintptr_t) size;\n",
    "\n",
    "    while (ptr < end) {\n",
    "        uint32_t cmd = *(uint32_t *) ptr;\n",
    "        ptr += sizeof(uint32_t);\n",
    "        switch(cmd) {\n",
    "        case BR_TRANSACTION: {\n",
    "            struct binder_transaction_data *txn = (struct binder_transaction_data *) ptr;\n",
    "            ...\n",
    "            binder_dump_txn(txn);\n",
    "            if (func) {\n",
    "                unsigned rdata[256/4];\n",
    "                struct binder_io msg;\n",
    "                struct binder_io reply;\n",
    "                int res;\n",
    "\n",
    "                bio_init(&reply, rdata, sizeof(rdata), 4);\n",
    "                bio_init_from_txn(&msg, txn); //从txn解析出binder_io信息\n",
    "                 // 收到Binder事务 【见小节5.2】\n",
    "                res = func(bs, txn, &msg, &reply);\n",
    "                // 发送reply事件【见小节5.4】\n",
    "                binder_send_reply(bs, &reply, txn->data.ptr.buffer, res);\n",
    "            }\n",
    "            ptr += sizeof(*txn);\n",
    "            break;\n",
    "        }\n",
    "        case : ...\n",
    "    }\n",
    "    return r;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 svcmgr_handler\n",
    "```\n",
    "int svcmgr_handler(struct binder_state *bs, struct binder_transaction_data *txn, struct binder_io *msg, struct binder_io *reply) {\n",
    "    struct svcinfo *si;\n",
    "    uint16_t *s;\n",
    "    size_t len;\n",
    "    uint32_t handle;\n",
    "    uint32_t strict_policy;\n",
    "    int allow_isolated;\n",
    "    ...\n",
    "    strict_policy = bio_get_uint32(msg);\n",
    "    s = bio_get_string16(msg, &len);\n",
    "    ...\n",
    "\n",
    "    switch(txn->code) {\n",
    "      case SVC_MGR_ADD_SERVICE:\n",
    "          s = bio_get_string16(msg, &len);\n",
    "          ...\n",
    "          handle = bio_get_ref(msg); //获取handle\n",
    "          allow_isolated = bio_get_uint32(msg) ? 1 : 0;\n",
    "           //注册指定服务 【见小节5.3】\n",
    "          if (do_add_service(bs, s, len, handle, txn->sender_euid,\n",
    "              allow_isolated, txn->sender_pid))\n",
    "              return -1;\n",
    "          break;\n",
    "       case :...\n",
    "    }\n",
    "\n",
    "    bio_put_uint32(reply, 0);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 do_add_service\n",
    "- svcinfo 记录着服务名和 handle 信息，保存到 svclist 列表\n",
    "\n",
    "```\n",
    "int do_add_service(struct binder_state *bs,\n",
    "                   const uint16_t *s, size_t len,\n",
    "                   uint32_t handle, uid_t uid, int allow_isolated,\n",
    "                   pid_t spid)\n",
    "{\n",
    "    struct svcinfo *si;\n",
    "\n",
    "    if (!handle || (len == 0) || (len > 127))\n",
    "        return -1;\n",
    "\n",
    "    // 权限检查\n",
    "    if (!svc_can_register(s, len, spid)) {\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // 服务检索\n",
    "    si = find_svc(s, len);\n",
    "    if (si) {\n",
    "        if (si->handle) {\n",
    "            svcinfo_death(bs, si); // 服务已注册时，释放相应的服务\n",
    "        }\n",
    "        si->handle = handle;\n",
    "    } else {\n",
    "        si = malloc(sizeof(*si) + (len + 1) * sizeof(uint16_t));\n",
    "        if (!si) {\n",
    "            // 内存不足，无法分配足够内存\n",
    "            return -1;\n",
    "        }\n",
    "        si->handle = handle;\n",
    "        si->len = len;\n",
    "        memcpy(si->name, s, (len + 1) * sizeof(uint16_t));  // 内存拷贝服务信息\n",
    "        si->name[len] = '\\0';\n",
    "        si->death.func = (void*) svcinfo_death;\n",
    "        si->death.ptr = si;\n",
    "        si->allow_isolated = allow_isolated;\n",
    "        si->next = svclist;   // svclist 保存所有已注册的服务\n",
    "        svclist = si;\n",
    "    }\n",
    "\n",
    "    // 以 BC_ACQUIRE 命令，handle 为目标的信息，通过 ioctl 发送给 binder 驱动\n",
    "    binder_acquire(bs, handle);\n",
    "    // 以 BC_REQUEST_DEATH_NOTIFICATION 命令的信息，通过 ioctl 发送给 binder 驱动，主要用于清理内存等收尾工作。\n",
    "    binder_link_to_death(bs, handle, &si->death);\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 binder_send_reply\n",
    "- binder_write 进入 binder 驱动后，将 BC_FREE_BUFFER 和 BC_REPLY 命令协议发送给 Binder 驱动， 向 client 端发送 reply\n",
    "\n",
    "```\n",
    "void binder_send_reply(struct binder_state *bs, struct binder_io *reply, binder_uintptr_t buffer_to_free, int status) {\n",
    "    struct {\n",
    "        uint32_t cmd_free;\n",
    "        binder_uintptr_t buffer;\n",
    "        uint32_t cmd_reply;\n",
    "        struct binder_transaction_data txn;\n",
    "    } __attribute__((packed)) data;\n",
    "\n",
    "    data.cmd_free = BC_FREE_BUFFER; // free buffer 命令\n",
    "    data.buffer = buffer_to_free;\n",
    "    data.cmd_reply = BC_REPLY;      // reply命令\n",
    "    data.txn.target.ptr = 0;\n",
    "    data.txn.cookie = 0;\n",
    "    data.txn.code = 0;\n",
    "    if (status) {\n",
    "        ...\n",
    "    } else {\n",
    "        data.txn.flags = 0;\n",
    "        data.txn.data_size = reply->data - reply->data0;\n",
    "        data.txn.offsets_size = ((char*) reply->offs) - ((char*) reply->offs0);\n",
    "        data.txn.data.ptr.buffer = (uintptr_t)reply->data0;\n",
    "        data.txn.data.ptr.offsets = (uintptr_t)reply->offs0;\n",
    "    }\n",
    "    // 向 Binder 驱动通信\n",
    "    binder_write(bs, &data, sizeof(data));\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 总结\n",
    "- 服务注册过程(addService)核心功能：在服务所在进程创建 binder_node，在 servicemanager 进程创建 binder_ref。 其中 binder_ref 的 desc 在同一个进程内是唯一的：\n",
    "  - 每个进程 binder_proc 所记录的 binder_ref 的 handle 值是从1开始递增的\n",
    "  - 所有进程 binder_proc 所记录的 handle = 0 的 binder_ref 都指向 service manager\n",
    "  - 同一个服务的 binder_node 在不同进程的 binder_ref 的 handle 值可以不同\n",
    "  \n",
    "  \n",
    "- Media 服务注册的过程涉及到 MediaPlayerService(作为 Client 进程)和 Service Manager(作为 Service 进程)，通信流程图如下所示：\n",
    "![image](binder5_page3.png)\n",
    "\n",
    "- 过程分析：\n",
    "  1. MediaPlayerService 进程调用 ioctl() 向 Binder 驱动发送 IPC 数据，该过程可以理解成一个事务 binder_transaction(记为T1)，执行当前操作的线程 binder_thread(记为thread1)，则 T1->from_parent = NULL，T1->from = thread1，thread1->transaction_stack = T1。其中 IPC 数据内容包含：\n",
    "     - Binder 协议为 BC_TRANSACTION\n",
    "     - Handle 等于 0\n",
    "     - RPC 代码为 ADD_SERVICE\n",
    "     - RPC 数据为\"media.player\"\n",
    "  2. Binder 驱动收到该 Binder 请求，生成 BR_TRANSACTION 命令，选择目标处理该请求的线程，即 ServiceManager 的 binder 线程(记为thread2)，则 T1->to_parent = NULL，T1->to_thread = thread2。并将整个 binder_transaction 数据(记为 T2)插入到目标线程的 todo 队列\n",
    "  3. Service Manager 的线程 thread2 收到 T2 后，调用服务注册函数将服务\"media.player\"注册到服务目录中。当服务注册完成后，生成 IPC 应答数据(BC_REPLY)，T2->form_parent = T1，T2->from = thread2, thread2->transaction_stack = T2\n",
    "  4. Binder 驱动收到该 Binder 应答请求，生成 BR_REPLY 命令，T2->to_parent = T1，T2->to_thread = thread1, thread1->transaction_stack = T2。 在 MediaPlayerService 收到该命令后，知道服务注册完成便可以正常使用\n",
    "  \n",
    "> 整个过程中，BC_TRANSACTION 和 BR_TRANSACTION 过程是一个完整的事务过程；BC_REPLY 和 BR_REPLY 是一个完整的事务过程"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
